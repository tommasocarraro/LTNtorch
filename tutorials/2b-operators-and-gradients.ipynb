{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Complementary Notebook: Appropriate Operators to Approximate Connectives and Quantifiers\n",
    "\n",
    "This notebook is a complement to the tutorial on operators (2-grounding_connectives.ipynb).\n",
    "\n",
    "Logical connectives are grounded in LTN using fuzzy semantics. However, while all fuzzy logic operators make sense when simply *querying* the language, not every operator is equally suited for *learning*.\n",
    "\n",
    "We will see common problems of some fuzzy semantics and which operators are better for the task of *learning*."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import ltn\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Querying\n",
    "\n",
    "One can access the implementation of the most common fuzzy semantics in the `ltn.fuzzy_ops` module.\n",
    "They are implemented using PyTorch primitives.\n",
    "\n",
    "Here, we compare:\n",
    "- the product t-norm: $u \\land_{\\mathrm{prod}} v = uv$,\n",
    "- the Lukasiewicz t-norm: $u \\land_{\\mathrm{luk}} v = \\max(u+v-1,0)$,\n",
    "- the minimum aggregator: $\\min(u_1,\\dots,u_n)$,\n",
    "- the p-mean error aggregator (generalized mean of the deviations w.r.t. the truth): $\\mathrm{pME}(u_1,\\dots,u_n) = 1 - \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n (1-u_i)^p \\biggr)^{\\frac{1}{p}}$.\n",
    "\n",
    "Each operator obviously conveys very different meanings, but they can all make sense depending on the intent of the query.\n",
    "\n",
    "In the following, it is possible to observe that different semantics for the conjunction return very different results.\n",
    "The same behavior can be observed when comparing different aggregators computed on the same input."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2800)\n",
      "tensor(0.1000)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor(0.4)\n",
    "x2 = torch.tensor(0.7)\n",
    "\n",
    "# the stable keyword is explained at the end of the notebook\n",
    "and_prod = ltn.fuzzy_ops.AndProd(stable=False)\n",
    "and_luk = ltn.fuzzy_ops.AndLuk()\n",
    "\n",
    "print(and_prod(x1, x2))\n",
    "print(and_luk(x1, x2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1000)\n",
      "tensor(0.3134)\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor([1., 1., 1., 0.5, 0.3, 0.2, 0.2, 0.1])\n",
    "\n",
    "# the stable keyword is explained at the end of the notebook\n",
    "forall_min = ltn.fuzzy_ops.AggregMin()\n",
    "forall_pME = ltn.fuzzy_ops.AggregPMeanError(p=4, stable=False)\n",
    "\n",
    "print(forall_min(xs, dim=0))\n",
    "print(forall_pME(xs, dim=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While all operators are suitable in a querying setting, this not the case in a learning setting. Indeed, many fuzzy logic operators have derivatives not suitable for gradient-based algorithms. For more details, read [van Krieken et al., *Analyzing Differentiable Fuzzy Logic Operators*, 2020](https://arxiv.org/abs/2002.06100).\n",
    "\n",
    "Here, we give simple illustrations of such gradient issues.\n",
    "\n",
    "#### 1. Vanishing Gradients\n",
    "\n",
    "Some operators have vanishing gradients on some part of their domains.\n",
    "\n",
    "For example, in $u \\land_{\\mathrm{luk}} v = \\max(u+v-1,0)$, if $u+v-1 < 0$, the gradients vanish.\n",
    "\n",
    "In the following, it is possible to observe an edge case in which the Lukasiewicz conjunction leads to vanishing gradients."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[tensor(0.), tensor(0.)]\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor(0.3, requires_grad=True)\n",
    "x2 = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "y = and_luk(x1, x2)\n",
    "y.backward()  # this is necessary to compute the gradients\n",
    "res = y.item()\n",
    "gradients = [v.grad for v in [x1, x2]]\n",
    "# print the result of the aggregation\n",
    "print(res)\n",
    "# print gradients of x1 and x2\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Single-Passing Gradients\n",
    "\n",
    "Some operators have gradients propagating to only one input at a time, meaning that all other inputs will not benefit from learning at this step.\n",
    "\n",
    "An example is the minimum aggregator, namely $\\min(u_1,\\dots,u_n)$.\n",
    "\n",
    "In the following, it is possible to observe an edge case in which the `Min` aggregator leads to singe-passing gradients."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10000000149011612\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor([1., 1., 1., 0.5, 0.3, 0.2, 0.2, 0.1], requires_grad=True)\n",
    "\n",
    "y = forall_min(xs, dim=0)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print the result of the aggregation\n",
    "print(res)\n",
    "# print gradients of xs\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Exploding Gradients\n",
    "\n",
    "Some operators have exploding gradients on some part of their domains.\n",
    "\n",
    "An example is the `PMean` aggregator, namely $\\mathrm{pME}(u_1,\\dots,u_n) = 1 - \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n (1-u_i)^p \\biggr)^{\\frac{1}{p}}$.\n",
    "\n",
    "In the edge case where all inputs are $1.0$, this operator leads to exploding gradients.\n",
    "\n",
    "In the following, it is possible to observe this behavior."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "tensor([nan, nan, nan])\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "\n",
    "y = forall_pME(xs, dim=0, p=4)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print the result of the aggregation\n",
    "print(res)\n",
    "# print the gradients of xs\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stable Product Configuration\n",
    "\n",
    "#### Product Configuration\n",
    "\n",
    "In general, we recommend using the following \"product configuration\" in LTN:\n",
    "* not: the standard negation  $\\lnot u = 1-u$,\n",
    "* and: the product t-norm $u \\land v = uv$,\n",
    "* or: the product t-conorm (probabilistic sum) $u \\lor v = u+v-uv$,\n",
    "* implication: the Reichenbach implication $u \\rightarrow v = 1 - u + uv$,\n",
    "* existential quantification (\"exists\"): the generalized mean (p-mean) $\\mathrm{pM}(u_1,\\dots,u_n) = \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n u_i^p \\biggr)^{\\frac{1}{p}} \\qquad p \\geq 1$,\n",
    "* universal quantification (\"for all\"): the generalized mean of \"the deviations w.r.t. the truth\" (p-mean error) $\\mathrm{pME}(u_1,\\dots,u_n) = 1 - \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n (1-u_i)^p \\biggr)^{\\frac{1}{p}} \\qquad p \\geq 1$.\n",
    "\n",
    "#### \"Stable\"\n",
    "\n",
    "As is, this \"product configuration\" is not fully exempt from issues:\n",
    "- the product t-norm has vanishing gradients on the edge case $u=v=0$;\n",
    "- the product t-conorm has vanishing gradients on the edge case $u=v=1$;\n",
    "- the Reichenbach implication has vanishing gradients on the edge case $u=0$,$v=1$;\n",
    "- `pMean` has exploding gradients on the edge case $u_1=\\dots=u_n=0$;\n",
    "- `pMeanError` has exploding gradients on the edge case $u_1=\\dots=u_n=1$.\n",
    "\n",
    "However, all these issues happen on edge cases and can easily be fixed using the following \"trick\":\n",
    "- if the edge case happens when an input $u$ is $0$, we modify every input with $u' = (1-\\epsilon)u+\\epsilon$;\n",
    "- if the edge case happens when an input $u$ is $1$, we modify every input with $u' = (1-\\epsilon)u$;\n",
    "\n",
    "where $\\epsilon$ is a small positive value (e.g. $1\\mathrm{e}{-5}$).\n",
    "\n",
    "This \"trick\" gives us a stable version of such operators. Stable in the sense it has not gradient issues anymore.\n",
    "\n",
    "One can trigger the stable version of such operators by using the boolean parameter `stable`. It is possible to set a default\n",
    "value for `stable` when initializing the operator, or to use different values at each call of the operator.\n",
    "\n",
    "In the following, we repeat the last example with the difference that we are now using the stable version of the `pMean`\n",
    "operator. It is possible to observe that the gradients are now different from `NaN`. Thanks to the stable verison of the\n",
    "operator, we are now able to obtain suitable gradients."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998999834060669\n",
      "tensor([0.3333, 0.3333, 0.3333])\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "\n",
    "# the exploding gradient problem is solved\n",
    "y = forall_pME(xs, dim=0, p=4, stable=True)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print the result of the aggregation\n",
    "print(res)\n",
    "# print the gradients of xs\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The hyper-parameter $p$ in the generalized means\n",
    "\n",
    "The hyper-parameter $p$ of `pMean` and `pMeanError` offers flexibility in writing more or less strict formulas, to\n",
    "account for outliers in the data depending on the application. However, $p$ should be carefully set since it could have\n",
    "strong implications for the training of LTN.\n",
    "\n",
    "In the following, we see how a huge increase of $p$ leads to single-passing gradients in the `pMean` operator. This is\n",
    "intuitive as in the second tutorial we have observed that `pMean` tends to the `Max` when $p$ tends to infinity. Similar\n",
    "to the `Min` aggregator (seen before in this tutorial), the `Max` aggregator leads to single-passing gradients."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31339913606643677\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0483, 0.1325, 0.1977, 0.1977, 0.2815])\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor([1., 1., 1., 0.5, 0.3, 0.2, 0.2, 0.1], requires_grad=True)\n",
    "\n",
    "y = forall_pME(xs, dim=0, p=4)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print result of aggregation\n",
    "print(res)\n",
    "# print gradients of xs\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18157517910003662\n",
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0734e-05, 6.4147e-03, 8.1100e-02,\n",
      "        8.1100e-02, 7.6019e-01])\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor([1., 1., 1., 0.5, 0.3, 0.2, 0.2, 0.1], requires_grad=True)\n",
    "\n",
    "y = forall_pME(xs, dim=0, p=20)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print result of aggregation\n",
    "print(res)\n",
    "# print gradients of xs\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While it can be tempting to set a high value for $p$ when querying, in a learning setting, this can quickly lead to a \"single-passing\" operator that will focus too much on outliers at each step (i.e., gradients overfitting one input at this step, potentially harming the training of the others). We recommend not to set a too high $p$ when learning.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}